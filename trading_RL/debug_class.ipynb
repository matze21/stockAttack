{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import py_environment\n",
    "\n",
    "\n",
    "class StockAttackEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(5,), dtype=np.float64, minimum=0, name='observation')\n",
    "    self._episode_ended        = False\n",
    "    self.episodeTimeStepNumber = 0\n",
    "    self.capital               = np.random.randint(0,2)\n",
    "    self.ownedStock            = np.random.randint(0,2)\n",
    "    self.lengthData            = 5\n",
    "    self.data                  = np.ones(self.lengthData) * self.ownedStock  #data is random either all 1 or all 0\n",
    "    self.changePosition        = np.random.randint(0, self.lengthData)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    self.data[self.changePosition: self.lengthData] = np.ones(self.lengthData - self.changePosition) * np.random.randint(0,2) # data can either have a positive or negative jump, or no change at all\n",
    "    self.cost                  = 0\n",
    "    self.portfolioValue        = self.capital + self.ownedStock * self.data[0]\n",
    "    self.initialPortfolioValue = self.capital + self.ownedStock * self.data[0]\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self.capital               = np.random.randint(0,2)\n",
    "    self.ownedStock            = np.random.randint(0,2)\n",
    "    self.data                  = np.ones(self.lengthData) * self.ownedStock  #data is random either all 1 or all 0\n",
    "    self.changePosition        = np.random.randint(0, self.lengthData)\n",
    "    self.data[self.changePosition: self.lengthData] = np.ones(self.lengthData - self.changePosition) * np.random.randint(0,2) # data can either have a positive or negative jump, or no change at all\n",
    "    self.cost                  = 0\n",
    "    self.portfolioValue        = self.capital + self.ownedStock * self.data[0]\n",
    "    self._episode_ended        = False\n",
    "    self.episodeTimeStepNumber = 0\n",
    "    self._observation_spec     = [self.data[self.episodeTimeStepNumber], self.data[self.episodeTimeStepNumber], 0, self.capital, self.ownedStock]\n",
    "    self.initialPortfolioValue = self.capital + self.ownedStock * self.data[0]\n",
    "    return ts.restart(np.array(self._observation_spec))\n",
    "\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "\n",
    "    price_buy  = self.data[self.episodeTimeStepNumber]+self.cost\n",
    "    price_sell = self.data[self.episodeTimeStepNumber]-self.cost\n",
    "    plausible  = True\n",
    "\n",
    "    \n",
    "    if action == 1:\n",
    "        plausible = (self.ownedStock == 0) and (self.capital >= price_buy)\n",
    "        if plausible:\n",
    "            self.capital    -= price_buy\n",
    "            self.ownedStock  = 1\n",
    "    elif action == 0:\n",
    "        pass\n",
    "    elif action == 2:\n",
    "        plausible = (self.ownedStock == 1)\n",
    "        if plausible:\n",
    "            self.capital   += price_sell * self.ownedStock\n",
    "            self.ownedStock = 0\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    \n",
    "    self.portfolioValue = self.capital + self.ownedStock * price_sell\n",
    "    plausbilityPenalty  = 0\n",
    "    if not plausible:\n",
    "        plausbilityPenalty = 1\n",
    "    reward = self.portfolioValue - plausbilityPenalty - self.initialPortfolioValue\n",
    "\n",
    "\n",
    "    self._action_spec = [action]\n",
    "    if self.episodeTimeStepNumber >= 1:\n",
    "        self._observation_spec = [self.data[self.episodeTimeStepNumber-1], self.data[self.episodeTimeStepNumber], plausbilityPenalty, self.capital, self.ownedStock]\n",
    "    else:\n",
    "        self._observation_spec = [self.data[self.episodeTimeStepNumber], self.data[self.episodeTimeStepNumber], plausbilityPenalty, self.capital, self.ownedStock]\n",
    "\n",
    "        \n",
    "    if self.data.shape[0]-1>self.episodeTimeStepNumber:\n",
    "        self.episodeTimeStepNumber +=1\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return ts.transition(np.array(self._observation_spec), reward, discount=0.0)\n",
    "    else:\n",
    "      self._episode_ended = True\n",
    "      return ts.termination(np.array(self._observation_spec), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = StockAttackEnv()\n",
    "action = np.array([2])\n",
    "time_step = environment.reset()\n",
    "stocks = []\n",
    "while not time_step.is_last():\n",
    "  time_step = environment.step(action)\n",
    "  stocks.append(environment.ownedStock)\n",
    "#  print(time_step)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(environment.data)\n",
    "#plt.plot(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "    \n",
    "\n",
    "def collect_episode(environment, policy, replay_buffer, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "## setup agent\n",
    "learning_rate = 1e-3\n",
    "numerNeurons = 100\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "env = tf_py_environment.TFPyEnvironment(StockAttackEnv())\n",
    "env.reset()\n",
    "\n",
    "fc_layer_params = (numerNeurons,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replay buffer\n",
    "replay_buffer_max_length = 1000\n",
    "batch_size = 64\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple training agent\n",
    "num_eval_episodes = 10\n",
    "num_iterations = 10000\n",
    "collect_steps_per_iteration = 2\n",
    "log_interval  = 200\n",
    "eval_interval = 200\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(returns)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_episode(env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "  iterator = iter(dataset)\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(environment, policy):\n",
    "  total_return = 0.0\n",
    "  time_step = environment.reset()\n",
    "\n",
    "  data = []\n",
    "  rewards = []\n",
    "  actions = []\n",
    "  ownedStocks = []\n",
    "  ownedCapital = []\n",
    "  while not time_step.is_last():\n",
    "    action_step = policy.action(time_step)\n",
    "    time_step   = environment.step(action_step.action)\n",
    "    total_return += time_step.reward\n",
    "    actions.append(action_step.action)\n",
    "    rewards.append(time_step.reward)\n",
    "    data.append(time_step.observation.numpy()[0][1])\n",
    "    ownedStocks.append(time_step.observation.numpy()[0][4])\n",
    "    ownedCapital.append(time_step.observation.numpy()[0][3])\n",
    "\n",
    "  ax1 = plt.subplot(5,1,1)\n",
    "  ax1.plot(data)\n",
    "  ax1.set_title('data')\n",
    "  ax2 = plt.subplot(5,1,2)\n",
    "  ax2.plot(ownedCapital)\n",
    "  ax2.set_title('capital')\n",
    "  ax3 = plt.subplot(5,1,3)\n",
    "  ax3.plot(ownedStocks)\n",
    "  ax3.set_title('owned stock')\n",
    "  ax4 = plt.subplot(5,1,4)\n",
    "  ax4.plot(actions)\n",
    "  ax4.set_title('actions')\n",
    "  ax5 = plt.subplot(5,1,5)\n",
    "  ax5.plot(rewards)\n",
    "  ax5.set_title('rewards')\n",
    "  return total_return.numpy()[0]\n",
    "\n",
    "plt.plot(returns)\n",
    "evaluate_policy(env, agent.policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
