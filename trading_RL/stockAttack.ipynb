{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import yahoofinancials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "#import pyvirtualdisplay\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_test = yf.download('TSLA', \n",
    "                      start='2020-06-01', \n",
    "                      end='2020-06-02',\n",
    "                     interval = '1m')\n",
    "#tsla_test['Close'].plot()\n",
    "print(tsla_test.shape)\n",
    "print(tsla_test.index[tsla_test.shape[0]-1])\n",
    "tsla_mod = tsla_test.drop(tsla_test.index[tsla_test.shape[0]-1])\n",
    "print(tsla_mod.shape)\n",
    "#help(tsla_test)\n",
    "#help(yf.ticker)\n",
    "print(tsla_mod)\n",
    "tsla_mod['Close'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import py_environment\n",
    "\n",
    "# ticker  = yf.Ticker('TSLA')\n",
    "# tsla_df = ticker.history(period=\"max\")\n",
    "#'data = tsla_mod['Close']\n",
    "\n",
    "class StockAttackEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._episode_ended = False\n",
    "    self.episodeTimeStepNumber = 0\n",
    "    self.initCapital           = 1000\n",
    "    self._state                 = self.initCapital\n",
    "    self.ownedStock            = 0\n",
    "    self.data = tsla_mod['Close']\n",
    "    self.cost = 1\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = self.initCapital #start with value of 100\n",
    "    self._episode_ended = False\n",
    "    self.episodeTimeStepNumber = 0\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "    \n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    #define observation = current stock price, ATTENTION obseration could be known values?\n",
    "    self._observation_spec = self.data[self.episodeTimeStepNumber]\n",
    "    #self._observation_spec = self.episodeTimeStepNumber\n",
    "    \n",
    "    # Make sure episodes don't go on forever.\n",
    "    price_buy  = self.data[self.episodeTimeStepNumber]+self.cost\n",
    "    price_sell = self.data[self.episodeTimeStepNumber]-self.cost\n",
    "    if action == 1:\n",
    "    #start out with buying/selling always the max amount of stocks\n",
    "        howManyStocksCanIAfford = (self._state-self._state%price_buy)/price_buy\n",
    "        if howManyStocksCanIAfford > 0:\n",
    "            self._state = self._state - (price_buy - self.cost) *howManyStocksCanIAfford\n",
    "            self.ownedStock = self.ownedStock + howManyStocksCanIAfford\n",
    "            \n",
    "            \n",
    "    elif action == 0:\n",
    "      #do nothing\n",
    "        pass\n",
    "    elif action == 2:\n",
    "        self._state     = self._state + price_sell * self.ownedStock\n",
    "        self.ownedStock = 0\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    reward = self._state + price_sell * self.ownedStock - self.initCapital\n",
    "    if self.data.shape[0]-1>self.episodeTimeStepNumber:\n",
    "        self.episodeTimeStepNumber +=1\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return ts.transition(np.array([self._state], dtype=np.int32), reward, discount=0.0)\n",
    "    else:\n",
    "      self._episode_ended = True\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "\n",
    "    #print(action)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work, no clue why\n",
    "environment = StockAttackEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure if reward has to cumulative, if yes make reward the difference to the init state\n",
    "#e.g. init state = 1000, current state = 1100  reward = +100\n",
    "\n",
    "buy_action = np.array(1, dtype=np.int32)\n",
    "sell_action = np.array(2, dtype=np.int32)\n",
    "wait_action = np.array(0, dtype=np.int32)\n",
    "\n",
    "environment = StockAttackEnv()\n",
    "time_step = environment.reset()\n",
    "#print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "current_observation = environment.observation_spec()\n",
    "print(cumulative_reward, current_observation)\n",
    "\n",
    "time_step = environment.step(buy_action)\n",
    "#print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "current_observation = environment.observation_spec()\n",
    "print(cumulative_reward, current_observation)\n",
    "\n",
    "time_step = environment.step(wait_action)\n",
    "cumulative_reward += time_step.reward\n",
    "current_observation = environment.observation_spec()\n",
    "print(cumulative_reward, current_observation)\n",
    "\n",
    "time_step = environment.step(sell_action)\n",
    "#print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "current_observation = environment.observation_spec()\n",
    "print(cumulative_reward, current_observation)\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simple environment\n",
    "\n",
    "#parameters\n",
    "num_iterations = 20 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 20  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 100  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 16  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 1  # @param {type:\"integer\"}\n",
    "eval_interval = 1  # @param {type:\"integer\"}\n",
    "\n",
    "PyEnv = StockAttackEnv()\n",
    "env = tf_py_environment.TFPyEnvironment(PyEnv)\n",
    "env.reset()\n",
    "action_spec = PyEnv.action_spec()\n",
    "#print(action_spec.shape)\n",
    "\n",
    "fc_layer_params = (1000,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "print(agent.collect_data_spec)\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(avg_return)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(\n",
    "      env, agent.collect_policy, collect_steps_per_iteration)\n",
    "\n",
    "  dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "  iterator = iter(dataset)\n",
    "  iterator.next()\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  #experience = replay_buffer.gather_all()\n",
    "  experience, unused_info = next(iterator)\n",
    "  #import pdb; pdb.set_trace()\n",
    "  train_loss = agent.train(experience).loss\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "#plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
